{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e954ca92-ae79-46f4-b75c-c27c22f06954",
   "metadata": {},
   "source": [
    "# DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992c08af-b36e-4d9f-a4a8-b6214d334dfd",
   "metadata": {},
   "source": [
    "DataFrame w Spark to rozproszony zbiór danych zorganizowany w kolumny (jak tabela w bazie danych). Jest to podstawowa abstrakcja danych w Spark SQL i opiera się na koncepcji schematu danych + RDD. DataFrame to:\n",
    "\n",
    "- zbiór wierszy z nazwanymi kolumnami,\n",
    "\n",
    "- pozwala na operacje podobne do SQL: select, filter, groupBy, join, itd.,\n",
    "\n",
    "- umożliwia optymalizację przez Catalyst Optimizer i Tungsten Engine,\n",
    "\n",
    "- może być ładowany z różnych źródeł: CSV, Parquet, JSON, Hive, JDBC, itd."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d615ed6-6dd5-4b4c-87fa-7f0ca4abe9da",
   "metadata": {},
   "source": [
    "#### Tworzenie DataFrame z RDD z wykorzystaniem klasy Row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21565f2c-7f09-47cf-b825-66f0663a3500",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CreateDF_Row\").getOrCreate()\n",
    "\n",
    "rdd = spark.sparkContext.parallelize([\n",
    "    Row(jury=\"Julia Wieniawa\", age=23),\n",
    "    Row(jury=\"Agnieszka Chylińska\", age=49),\n",
    "    Row(jury=\"Marcin Prokop\", age=48),\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(rdd)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4974da-48f6-415b-a290-f03c164857f2",
   "metadata": {},
   "source": [
    "#### Z listy słowników (Python dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462791e9-60a0-4072-9b65-0add8144995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"jury\": \"Julia Wieniawa\", \"wiek\": 27},\n",
    "    {\"jury\": \"Agnieszka Chylińska\", \"wiek\": 49},\n",
    "    {\"jury\": \"Marcin Prokop\", \"wiek\": 48}\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3253315-472f-4df7-a965-528ee47622ee",
   "metadata": {},
   "source": [
    "#### Z listy tuple + podanie nazw kolumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c73a2c0-d90d-43cd-b176-3412da00ef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"Marcin Prokop\", 48), (\"Julia Wieniawa\", 27),(\"Agnieszka Chylińska\",49)]\n",
    "\n",
    "columns = [\"name\", \"age\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd741dc7-494c-4a8a-8b7d-6c15fa66ecc3",
   "metadata": {},
   "source": [
    "#### Z RDD i StructType + StructField (pełne typowanie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639f37d6-cf3d-4e44-a880-e231140be0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "data = [(\"Marcin Prokop\", 48), (\"Julia Wieniawa\", 27),(\"Agnieszka Chylińska\",49)]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "#Programistyczny sposób definiowania schematu -> Pozmieniajmy nieco schemat i dane\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(rdd, schema=schema)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949f519b-862f-4c3f-ae6a-b3948c593b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wykorzystanie ciągu tekstowego DDL do definiowania schematu\n",
    "schema = 'name STRING, age INT not null'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93e3548-e137-4e65-9535-53fe959ce99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(rdd, schema=schema)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52392d35-9145-4f91-a2e2-890401447150",
   "metadata": {},
   "source": [
    "####  Z plików (CSV, JSON, Parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7d7813-5204-410e-b79d-723859275858",
   "metadata": {},
   "source": [
    "##### Skrócona wersja: csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3c3e90-dfe2-49be-867a-b65fdc0adbee",
   "metadata": {},
   "source": [
    "- Wyraźnie określa format pliku (csv, json, parquet, itd.)\n",
    "\n",
    "- Pozwala ustawić więcej opcji (delimiter, nullValue, inferSchema, encoding, itd.)\n",
    "\n",
    "- Używana często w produkcji lub do bardziej złożonego ładowania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0788e14-8f68-4d04-8b50-526312765e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\").option(\"infeSchema\", \"true\").csv(\"data/flights/summary-data/csv/*\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee71a86-e79a-4f56-9bda-f905e5047040",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eea5325-9f14-419f-bc49-8df2d1843033",
   "metadata": {},
   "source": [
    "##### Rozszerzona wersja: format(\"csv\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9f7e63-7865-4b6a-b5b0-42663e1bab0d",
   "metadata": {},
   "source": [
    "- Wyraźnie określa format pliku (csv, json, parquet, itd.)\n",
    "\n",
    "- Pozwala ustawić więcej opcji (delimiter, nullValue, inferSchema, encoding, itd.)\n",
    "\n",
    "- Używana często w produkcji lub do bardziej złożonego ładowania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca95b04-1f30-44e9-9754-08d5c1f34eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights_csv = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(\"data/flights/summary-data/csv/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2ad01c-b27e-4a5e-9f50-d975c23afd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights_csv.schema.simpleString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878de818-c7ef-4f8e-8487-11e4067a5a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights_json = (spark.read\n",
    "        .format(\"json\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(\"data/flights/summary-data/json/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1631911b-1b5b-4e52-8207-42f77657087d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights_json.schema.simpleString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb24966b-2138-43ff-bbec-7c265dcf122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet to kolumnowy format pliku przechowujący dane tabelaryczne, zaprojektowany z myślą o efektywnym przechowywaniu i szybkim dostępie do dużych zbiorów danych. \n",
    "# W przeciwieństwie do formatów wierszowych (jak CSV), przechowuje dane według kolumn, co pozwala na optymalizację zapytań, kompresję i przyspieszenie odczytu tylko potrzebnych kolumn\n",
    "df_flights_parquet = (spark.read\n",
    "        .format(\"parquet\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(\"data/flights/summary-data/parquet/*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb00268-5683-4c16-aced-75be02a739d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights_parquet.schema.simpleString()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b13760-8555-4520-ac50-8a701b5e49f6",
   "metadata": {},
   "source": [
    "#### Użycie toDF() z RDD lub listy - mniej elastyczne niz  funkcja powyzej (nie podajemy schematu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03900a62-648a-4af4-83b5-5c0f521781a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize(data)\n",
    "df = rdd.toDF([\"name\", \"age\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128dc660-67f7-42e1-9f18-d6972bf872aa",
   "metadata": {},
   "source": [
    "#### Zadanie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6095a98c-3402-4b51-970c-e0cff329675a",
   "metadata": {},
   "source": [
    "> Stówrz dataframe o kolumnach `daty`,`temat`,`godziny` dla zajęć ze sparkiem\n",
    "\n",
    "**Hint** Dla daty wykorzystaj `datetime.datetime`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bce932-5a16-4990-9b42-7a76955b2dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774030b1-aff1-4af4-8013-efaecd2a9528",
   "metadata": {},
   "source": [
    "**Przykładowy wynik**\n",
    "\n",
    "```\n",
    "+----------+------------------+-------+\r\n",
    "|      date|             temat|godziny|\r\n",
    "+----------+------------------+-------+\r\n",
    "|2025-05-30|       Spark - RDD|      3|\r\n",
    "|2025-05-30|   Spark-Dataframe|      3|\r\n",
    "|2025-05-30|Spark-kolejny krok|      3|\r\n",
    "+----------+------------------+-------+\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7994847b-a62d-431f-b721-f62ec44d2e57",
   "metadata": {},
   "source": [
    "## Podstawowe metody do przeglądania struktury DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f339cf-a164-4833-a02b-3d13b067f28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wyświetla schemat DataFrame w formacie drzewa.\n",
    "df_flights_parquet.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b7cc59-1e89-44d3-8c7f-426766b1d911",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zwraca obiekt StructType, który reprezentuje schemat DataFrame.\n",
    "df_flights_parquet.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbac9dff-9971-4121-9858-2ae94c558cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zwraca listę krotek (nazwa_kolumny, typ_danych).\n",
    "df_flights_parquet.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45fbfcd-a6c2-477a-a824-98e52828a150",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zwraca listę nazw kolumn.\n",
    "df_flights_parquet.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41c2932-8cd9-42ed-a095-0ad551f43391",
   "metadata": {},
   "source": [
    "To **atrybut obiektu**, czyli bezpośredni dostęp do kolumny tylko wtedy, gdy jej nazwa jest poprawnym identyfikatorem języka Python (np. bez spacji, nie zaczyna się od cyfry itd.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8198c24a-96f4-4eff-9e98-1f97843b55a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights_parquet.DEST_COUNTRY_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631849d-a75b-4fc4-8521-27a3a8ed0dd6",
   "metadata": {},
   "source": [
    "To **indeksowanie kolumny po nazwie** – zalecana metoda, ponieważ:\n",
    "\n",
    "- Działa zawsze – niezależnie od tego, jaką nazwę ma kolumna.\n",
    "\n",
    "- Jest bardziej bezpieczna i odporna na błędy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32b15b8-cbc4-4e0d-9331-c644cc23ba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights_parquet['DEST_COUNTRY_NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a21f99-4efa-44a5-a574-767a51ff46e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights_parquet[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb87d2d7-cd49-4848-9648-acaa37a818c9",
   "metadata": {},
   "source": [
    "Metoda .show() służy do wyświetlania danych z DataFrame'a w Spark w czytelnej, konsolowej formie tabeli. Jest to jedna z najczęściej używanych metod do szybkiego podglądu danych.\n",
    "\n",
    "**Składnia:**\n",
    "\n",
    "`df.show(n=20, truncate=True, vertical=False)\r\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c77e160-fb35-47ab-93b5-588f76f6702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights_parquet.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d4f30c-47ff-47c1-8b80-d2c33bb90d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Nie przycinaj zawartości kolumn:\n",
    "df_flights_parquet.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc05fcd4-2187-49d7-b461-b3c93d820f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# przydatne przy długich/nested danych\n",
    "df_flights_parquet.show(4,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63cb0d6-304c-41fb-919c-374a5a4116d0",
   "metadata": {},
   "source": [
    "### select()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf63ffe7-3bc9-42ec-9166-400162ea08a6",
   "metadata": {},
   "source": [
    "Metoda .select() w PySpark służy do wybierania jednej lub wielu kolumn z DataFrame’a. Można jej używać także do obliczeń, aliasów i wyrażeń z funkcji (F.col(), F.expr(), itp.).\n",
    "\n",
    "**Składnia:**\n",
    "```\n",
    "df.select(\"col1\", \"col2\", ...)\r\n",
    "df.select(df.col1, df.col2)\r\n",
    "df.select(F.col(\"col1\").alias(\"new_name\"))\r\n",
    "df.selectExpr(\"col1\", \"col2 + 1 as col2_plus_1\")\r\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91139e7-8880-41c0-9562-0dc92116a2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights_parquet.select(\"DEST_COUNTRY_NAME\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fab1321-ec45-4d1b-adb6-1d441d0c73a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Wybór wielu kolumn:\n",
    "df_flights_parquet.select(\"DEST_COUNTRY_NAME\",\"ORIGIN_COUNTRY_NAME\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701b4d8d-75e5-473c-8d3e-defeefed932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights_parquet.select(\"*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f723712-9797-405a-aa01-5e9533e3ab4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zmiana nazwy kolumny\n",
    "from pyspark.sql.functions import col\n",
    "df_flights_parquet.select(col(\"DEST_COUNTRY_NAME\").alias(\"kraj_docelowy\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954cea4b-99af-4c0a-95b3-e68435ac7c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "df_flights_parquet.select(df_flights_parquet.DEST_COUNTRY_NAME,\n",
    "                           df_flights_parquet.ORIGIN_COUNTRY_NAME,\n",
    "                           concat_ws(\" -> \", \"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").alias(\"połączenie\")\n",
    "                         ).show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d9c89c-e1c9-4f1d-9a01-27116001a9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights_parquet.selectExpr(\n",
    "    \"ORIGIN_COUNTRY_NAME\",\n",
    "    \"DEST_COUNTRY_NAME\",\n",
    "    \"concat(ORIGIN_COUNTRY_NAME, ' -> ', DEST_COUNTRY_NAME) as route\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0896623-40bf-4d41-a4c0-d6d48ff3bc6e",
   "metadata": {},
   "source": [
    "| Cecha               | `select()`                              | `selectExpr()`                        |\r\n",
    "| ------------------- | --------------------------------------- | ------------------------------------- |\r\n",
    "| Styl                | Pythonic (`col()`, `concat_ws()`, itp.) | SQL-like (stringi z wyrażeniami SQL)  |\r\n",
    "| Sprawdzanie składni | Błąd w czasie kompilacji                | Błąd w czasie wykonania (string eval) |\r\n",
    "| Złożone operacje    | Czasem dłuższe                          | Krócej, jeśli znasz SQL               |\r\n",
    "| Przykład aliasu     | `.alias(\"nazwa\")`                       | `\"kolumna as nazwa\"`                  |\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f8fea2-bfbd-4fea-bc37-a89df0b17976",
   "metadata": {},
   "source": [
    "### withColumn() - dodawanie lub modyfikowanie kolumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af7f05f-9fca-4edc-8355-f42a081e6ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights_parquet.withColumn(\"kraj_wylotu\", col(\"ORIGIN_COUNTRY_NAME\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a33f59-082e-42f1-ad1f-7b87844c1462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "#Modyfikację istniejącej kolumny:\n",
    "(df_flights_parquet\n",
    ".withColumn(\"ORIGIN_COUNTRY_NAME\", upper(col(\"ORIGIN_COUNTRY_NAME\")))\n",
    ".show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8772af4-3b72-4354-a388-26846c5d0ed8",
   "metadata": {},
   "source": [
    "### withColumnRenamed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83b31a2-8f00-450c-a755-0ee5308fe5f2",
   "metadata": {},
   "source": [
    "Metoda withColumnRenamed() w PySpark służy do zmiany nazwy istniejącej kolumny w DataFrame. Jest bardzo przydatna, gdy chcesz ustandaryzować nazwy kolumn lub przygotować dane do dalszych operacji.\n",
    "\n",
    "- `withColumnRenamed()` można wywołać w łańcuchu wielokrotnie, ale nie przyjmuje listy nazw (nie zmienia wielu kolumn na raz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2588b560-ed90-4ced-9436-3614bbbcb051",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_flights_parquet.withColumnRenamed(\"ORIGIN_COUNTRY_NAME\", \"Wylot\").withColumnRenamed(\"DEST_COUNTRY_NAME\",\"Przylot\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039b622f-44cf-4cf0-8a91-5e713625fdf1",
   "metadata": {},
   "source": [
    "### filter() where()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b12826c-1faa-4b83-84d3-1e906c9a6d93",
   "metadata": {},
   "source": [
    "W PySpark możesz filtrować wiersze w DataFrame za pomocą metod `filter()` oraz `where()` – i co ważne: działają one identycznie. To tylko różne sposoby zapisu tej samej operacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed23a22-6bf3-4f3b-8f18-4a42b885ca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights_parquet.filter(df_flights_parquet[\"DEST_COUNTRY_NAME\"] == \"United States\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac3ad11-cd44-4bda-b1ae-e770b50662d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights_parquet.where(df_flights_parquet[\"DEST_COUNTRY_NAME\"] == \"United States\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44e6afc-6e99-429f-b1a3-d58c7b58842b",
   "metadata": {},
   "source": [
    "Funkcja `isin()` w PySpark służy do sprawdzania, czy wartość w kolumnie należy do określonego zbioru wartości — odpowiednik SQL-owego IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f20199e-d2fc-4aa8-b715-e88bc005b5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights_parquet.filter(df_flights_parquet[\"ORIGIN_COUNTRY_NAME\"].isin(\"Portugal\", \"Costa Rica\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dccc50-466e-45da-98dc-da2fdfc21790",
   "metadata": {},
   "source": [
    "Łączenie warunków działa z operatorami:\n",
    "\n",
    "- & – logiczne AND\n",
    "\n",
    "- | – logiczne OR\n",
    "\n",
    "- ~ – NOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fba2245-04cd-44fc-a556-41ebfca4acb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Złożone warunki (AND, OR)\n",
    "df_flights_parquet.filter((df_flights_parquet[\"count\"] > 10) & (df_flights_parquet[\"ORIGIN_COUNTRY_NAME\"] == \"Canada\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5375aa-2a3e-45a6-9943-d08e8167b92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights_parquet.filter((col(\"count\") > 10) & (col(\"ORIGIN_COUNTRY_NAME\") == \"Canada\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5d8a62-254b-45f9-9047-9032c0c1f3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rlike\n",
    "\n",
    "# LIKE: nazwę kraju zaczynającego się od \"Uni\"\n",
    "df_flights_parquet.filter(df_flights_parquet[\"DEST_COUNTRY_NAME\"].like(\"Uni%\")).show()\n",
    "\n",
    "# RLIKE (regex): nazwy zawierające \"a\" i kończące się na \"a\"\n",
    "df_flights_parquet.filter(df_flights_parquet[\"ORIGIN_COUNTRY_NAME\"].rlike(\"a.*a$\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc43224-12d7-41ee-85ab-c3ed7caf09e2",
   "metadata": {},
   "source": [
    "#### Zadanie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819d219a-64e1-4bbf-8a98-2fa040476f62",
   "metadata": {},
   "source": [
    "Wyświetl tylko te rekordy, w których:\n",
    "\n",
    "- kraj pochodzenia `ORIGIN_COUNTRY_NAME` to Kanada lub Stany Zjednoczone\n",
    "ORAZ\n",
    "- liczba lotów (count) jest większa niż 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45351285-02c1-4e8f-a029-d2b8d616b0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04012f89-e835-42f0-a570-afa7e0a353a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cfe3d94a-1272-45a0-a3b7-858aa35883e6",
   "metadata": {},
   "source": [
    "**Przykładowy wynik**\n",
    "\n",
    " ```\n",
    "+------------------+-------------------+------+\n",
    "| DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
    "+------------------+-------------------+------+\n",
    "|            Mexico|      United States|  6200|\n",
    "|     United States|      United States|348113|\n",
    "|           Germany|      United States|  1392|\n",
    "|            Canada|      United States|  8271|\n",
    "|Dominican Republic|      United States|  1109|\n",
    "|             Japan|      United States|  1383|\n",
    "|    United Kingdom|      United States|  1629|\n",
    "|     United States|             Canada|  8305|\n",
    "+------------------+-------------------+------+\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a965e057-47ae-4991-b2f0-bea1dd6ddbde",
   "metadata": {},
   "source": [
    "#### Zadanie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f5737e-ded1-4b58-a80a-d8fbca79d953",
   "metadata": {},
   "source": [
    "Pokaż loty, gdzie kraj pochodzenia `ORIGIN_COUNTRY_NAME` NIE jest  z \"Russia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbee84d-2dea-4402-ab9b-265b4ea98033",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5890e40-0562-4836-b2c9-d3ae33aaec3f",
   "metadata": {},
   "source": [
    "### lit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f47aa93-a1ff-47fa-addc-696009f82560",
   "metadata": {},
   "source": [
    "Funkcja **lit()** w PySpark służy do tworzenia kolumny zawierającej stałą (literalną) wartość. Jest to szczególnie przydatne, gdy chcesz dodać kolumnę z tą samą wartością dla wszystkich wierszy lub użyć stałej w wyrażeniach kolumnowych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a637b85-72b0-418e-9b96-4cd932436797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df_flights_parquet.withColumn(\"stała_kolumna\", lit(1)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3228e3be-3cc7-49d3-a019-9a0b6c1fc1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights_parquet.withColumn(\"nowa_kolumna\", lit(None)).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffde26ec-dd5d-49ee-a647-c8c9f80d1267",
   "metadata": {},
   "source": [
    "### Manipulacji ciągami znaków"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e26b09-422d-46c6-bd64-69c5092a0da5",
   "metadata": {},
   "source": [
    "- `upper(col)`: Zamienia wszystkie znaki na wielkie litery.\n",
    "\n",
    "- `lower(col)`: Zamienia wszystkie znaki na małe litery.\n",
    "\n",
    "- `initcap(col)`: Zamienia pierwszy znak każdego słowa na wielką literę."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526c9e5f-ca6f-47a2-a174-9d798488812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import upper, lower, initcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb46587-8e67-4d46-9aca-e4a3eeef9294",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights_parquet.select(\n",
    "    upper(\"DEST_COUNTRY_NAME\").alias(\"kraj_docelowy_upper\"),\n",
    "    lower(\"DEST_COUNTRY_NAME\").alias(\"kraj_docelowy_lower\"),\n",
    "    initcap(\"DEST_COUNTRY_NAME\").alias(\"kraj_docelowy_initcap\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60df5bc7-358e-411b-a206-4d8019f8a983",
   "metadata": {},
   "source": [
    "**Łączenie ciągów znaków**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5034c916-ade5-49f4-86b7-d1d1f500b905",
   "metadata": {},
   "source": [
    "- `concat(col1, col2, ...)`: Łączy kolumny bez separatora.\n",
    "\n",
    "- `concat_ws(sep, col1, col2, ...)`: Łączy kolumny z podanym separatorem.\n",
    "\n",
    "- `format_string(format, *cols)`: Formatuje ciąg znaków w stylu printf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e9600d-a0a7-43a4-a535-385426e4bfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws, format_string\n",
    "\n",
    "df_flights_parquet.select(\n",
    "    concat_ws(\" \", \"DEST_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").alias(\"pełna_nazwa\")\n",
    ").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a87df4-026a-4752-9761-d5fe95c63c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights_parquet.select(format_string('%s => %s',\n",
    "                                        df_flights_parquet.ORIGIN_COUNTRY_NAME,\n",
    "                                        df_flights_parquet.DEST_COUNTRY_NAME)\n",
    "                          .alias(\"Wylot => Przylot\")).show(5,truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2f20bb-3ce1-4236-bb25-3fdb83c5277d",
   "metadata": {},
   "source": [
    "**Operacje na podciągach**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a1a4aa-42c0-4e2b-80f2-acb8762f9738",
   "metadata": {},
   "source": [
    "- `substring(col, pos, len)`: Zwraca podciąg od pozycji pos o długości len.\n",
    "\n",
    "- `instr(col, substr)`: Zwraca pozycję pierwszego wystąpienia substr w col.\n",
    "\n",
    "- `length()`:  Zwraca długości (liczby znaków) w stringu (tekście) w kolumnie DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074d4de1-60b6-4166-8fca-755cac505da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import substring, instr,length\n",
    "df_flights_parquet.select(\n",
    "    substring(df_flights_parquet.DEST_COUNTRY_NAME, 1, 5).alias(\"prefix\"),\n",
    "    instr(df_flights_parquet.DEST_COUNTRY_NAME, \"ed\").alias(\"ed_position\"),\n",
    "    length(df_flights_parquet.DEST_COUNTRY_NAME).alias(\"długość\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e178181d-f320-4cab-92ba-4e600609ebfc",
   "metadata": {},
   "source": [
    "**Czyszczenie i przycinanie**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbbf51b-0a41-416d-9aa9-bb2d06d584a3",
   "metadata": {},
   "source": [
    "- `trim(col)`: Usuwa białe znaki z początku i końca ciągu.\n",
    "\n",
    "- `ltrim(col)`, `rtrim(col)`: Usuwają białe znaki z lewej lub prawej strony.\n",
    "\n",
    "- `lpad(col, len, pad)`, `rpad(col, len, pad)`: Dopełniają ciąg do długości len znakiem pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8683c1-3832-441d-af18-7beadf7c5ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dane = [(\"   Spark.    \",) ]\n",
    "df = spark.createDataFrame(dane).toDF(\"test\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cee4c2-3cd8-457f-bd4d-6c210483e2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, ltrim, rtrim, trim\n",
    "df.withColumn(\"ltrim\", ltrim(col(\"test\"))). \\\n",
    "  withColumn(\"rtrim\", rtrim(col(\"test\"))). \\\n",
    "  withColumn(\"trim\", trim(col(\"test\"))). \\\n",
    "  show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c329aaa1-46a9-46f0-a4e9-fea81f39e9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, lpad,rpad\n",
    "\n",
    "df_flights_parquet.select(concat(\n",
    "    rpad(df_flights_parquet.ORIGIN_COUNTRY_NAME, 40, '-'),\n",
    "    lpad(df_flights_parquet.DEST_COUNTRY_NAME, 20,'-')\n",
    ").alias(\"Wylot => Przylot\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc33194-aa24-4b59-9c5e-14137ebe5a35",
   "metadata": {},
   "source": [
    "**Operacje z wyrażeniami regularnymi**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584947ae-cc0d-47e8-b40e-d5cbcac0526e",
   "metadata": {},
   "source": [
    "- `regexp_replace(col, pattern, replacement)`: Zamienia wszystkie wystąpienia wzorca pattern na replacement.\n",
    "\n",
    "- `regexp_extract(col, pattern, group)`: Wyodrębnia część ciągu pasującą do wzorca pattern.\n",
    "\n",
    "- `rlike(col, pattern)`: Sprawdza, czy ciąg pasuje do wyrażenia regularnego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c66624-166e-4761-90c7-e987f32f9ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "# Wyciągnij tylko pierwszy wyraz z ORIGIN_COUNTRY_NAME.\n",
    "df_flights_parquet.withColumn(\"first_word\", regexp_extract(\"ORIGIN_COUNTRY_NAME\", r\"^\\w+\", 0)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4459089b-8d0e-476a-8218-a38291b0a250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "#Zamień spacje na podkreślniki w DEST_COUNTRY_NAME\n",
    "df_flights_parquet.withColumn(\"dest_clean\", regexp_replace(\"DEST_COUNTRY_NAME\", \" \", \"_\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9676fcb5-979c-4c19-9f6a-93e21cf47c86",
   "metadata": {},
   "source": [
    "### dzielenia tekstu na części"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d1eb52-c3ea-4998-9750-65798835acd9",
   "metadata": {},
   "source": [
    "- `split(...)` – zwraca kolumnę typu array (tablica stringów),\n",
    "\n",
    "- `getItem(n)` – wybiera n-ty element (indeks od 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd616905-1f09-4959-87d9-4a7e1e8af6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "# rozdziel nazwy krajów\n",
    "df_flights_parquet.withColumn(\"pierwszy_człon\", split(\"DEST_COUNTRY_NAME\", \" \").getItem(0)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cdcd0e-8eda-43cb-88dd-baf58463788f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights_parquet.withColumn(\"drugi_człon\", split(\"DEST_COUNTRY_NAME\", \" \").getItem(1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb3adda-7219-4f0a-afbf-4de816047e31",
   "metadata": {},
   "source": [
    "### explode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a58c9e6-3ef6-4b5a-90a8-aff691b5d094",
   "metadata": {},
   "source": [
    "Funkcja `explode()` w PySpark jest używana do **\"rozpakowywania\" tablicy lub mapy** – tworzy osobny wiersz **dla każdego elementu w tablicy** w kolumnie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bcff2c-63c1-45b4-95b7-82db2d089a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df_flights_parquet.select(\n",
    "    \"DEST_COUNTRY_NAME\",\n",
    "    explode(split(\"DEST_COUNTRY_NAME\", \" \")).alias(\"word\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4e6b78-8ee4-4a68-9f3e-3ace7d3152ee",
   "metadata": {},
   "source": [
    "Jeśli **DEST_COUNTRY_NAME** to \"United States\", to powstaną 2 wiersze: \"United\" i \"States\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b74411-288e-4f4f-8ad7-9e4ed6317267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "data = [Row(imie=\"Tomek\", hobby=[\"sport\", \"gaming\"]),\n",
    "        Row(imie=\"Ola\", hobby=[\"podróże\", \"taniec\"])]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "df.select(\"imie\", explode(\"hobby\").alias(\"hobby\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a55741f-cd06-4583-8025-e7c6c104cd01",
   "metadata": {},
   "source": [
    "### funkcje daty/czasu w PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2cb612-0eac-49fa-a88a-3bd365c6cb53",
   "metadata": {},
   "source": [
    "| Funkcja                             | Opis                                          |\r\n",
    "| ----------------------------------- | --------------------------------------------- |\r\n",
    "| `current_date()`                    | Zwraca bieżącą datę                           |\r\n",
    "| `current_timestamp()`               | Zwraca bieżący znacznik czasu                 |\r\n",
    "| `to_date()`                         | Konwertuje string lub timestamp na `DateType` |\r\n",
    "| `to_timestamp()`                    | Konwertuje string na `TimestampType`          |\r\n",
    "| `date_format()`                     | Zwraca sformatowaną datę jako string          |\r\n",
    "| `datediff(date1, date2)`            | Liczba dni między datami (date1 - date2)      |\r\n",
    "| `months_between()`                  | Różnica miesięcy między datami                |\r\n",
    "| `year()`, `month()`, `dayofmonth()` | Ekstrakcja części daty                        |\r\n",
    "| `hour()`, `minute()`, `second()`    | Ekstrakcja części czasu                       |\r\n",
    "| `add_months(date, n)`               | Dodaje n miesięcy do daty                     |\r\n",
    "| `date_add(date, days)`              | Dodaje dni do daty                            |\r\n",
    "| `date_sub(date, days)`              | Odejmuje dni od daty                          |\r\n",
    "| `next_day(date, 'Mon')`             | Najbliższy podany dzień tygodnia              |\r\n",
    "| `trunc(date, 'MM')`                 | Ucina datę do początku miesiąca               |\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca9e13c-de72-4771-a6b1-a0c7cd93dcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp, to_date, year, month, weekofyear,dayofyear, dayofweek,hour, minute, second,to_timestamp,lit\n",
    "\n",
    "df = spark.range(1).select(\n",
    "    current_date().alias(\"dzisiaj\"),\n",
    "    current_timestamp().alias(\"teraz\")\n",
    ")\n",
    "\n",
    "(df.withColumn(\"year\", year(\"dzisiaj\"))\n",
    ".withColumn(\"month\", month(\"dzisiaj\"))\n",
    ".withColumn(\"weekofyear\", weekofyear(\"dzisiaj\"))\n",
    ".withColumn(\"dayofyear\", dayofyear(\"dzisiaj\"))\n",
    ".withColumn(\"weekday\", dayofweek(\"dzisiaj\"))\n",
    ".withColumn(\"hour\", hour(\"teraz\"))\n",
    ".withColumn(\"minute\", minute(\"teraz\"))\n",
    ".withColumn(\"second\", minute(\"teraz\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0b7d4c-90c7-4833-a2a5-89056b0c2a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datetime = spark.range(1).select(\n",
    "    to_date(lit('20250601'), 'yyyyMMdd').alias('to_date'),\n",
    "    to_timestamp(lit('20250601 1000'), 'yyyyMMdd HHmm').alias('to_timestamp'))\n",
    "df_datetime.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6dc211-a1ed-4e0c-87ee-275a2359b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub\n",
    "\n",
    "(df_datetime.withColumn(\"date_add_date\", date_add(\"to_date\", 10)).withColumn(\"date_add_time\", date_add(\"to_timestamp\", 10)).withColumn(\"date_sub_date\", date_sub(\"to_date\", 10)).withColumn(\"date_sub_time\", date_sub(\"to_timestamp\", 10)).show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce24bfd9-9340-42a1-b915-917c04bd2fa2",
   "metadata": {},
   "source": [
    "**date_format()**\n",
    "- data_lub_timestamp – kolumna typu DateType lub TimestampType\n",
    "\n",
    "- \"format\" – ciąg formatujący (np. \"yyyy-MM-dd\", \"MM/yyyy\", \"EEEE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859ed74f-2577-4a9c-ae31-1b955c3c60b4",
   "metadata": {},
   "source": [
    "| Symbol | Znaczenie            | Przykład |\r\n",
    "| ------ | -------------------- | -------- |\r\n",
    "| `yyyy` | rok                  | 2025     |\r\n",
    "| `MM`   | miesiąc (01-12)      | 05       |\r\n",
    "| `MMM`  | skrót miesiąca       | May      |\r\n",
    "| `MMMM` | pełna nazwa miesiąca | May      |\r\n",
    "| `dd`   | dzień miesiąca       | 26       |\r\n",
    "| `EEEE` | dzień tygodnia       | Monday   |\r\n",
    "| `HH`   | godzina (24h)        | 13       |\r\n",
    "| `mm`   | minuty               | 09       |\r\n",
    "| `ss`   | sekundy              | 47       |\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0eff6c-a199-448d-90c0-5e2e62cb9141",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a46a63-6604-45d2-a2a8-0a768dc0c617",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(\"2025-05-26\",)], [\"date_str\"])\n",
    "df = df.withColumn(\"date\", to_date(\"date_str\"))\n",
    "\n",
    "df.withColumn(\"formatted\", date_format(\"date\", \"MMMM yyyy\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b98bb4e-5f83-4639-833e-67de8108fe27",
   "metadata": {},
   "source": [
    "### Zadanie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3012da1-c8a5-4131-bd4b-767838157856",
   "metadata": {},
   "source": [
    "> Utwórz nową kolumnę z nazwa dnia tygodnia dla dzisiejszej daty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af367bac-c259-4230-badc-175abad1df50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.range(1).select(current_date().alias(\"today\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78c9fc6-1c38-462b-b66e-223cb82d4f98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34663260-1ba2-46e6-9c9c-2d11d15a8149",
   "metadata": {},
   "source": [
    "**Przykładowy wynik**\n",
    "\n",
    "```\n",
    "+----------+---------+\r\n",
    "|     today|  weekday|\r\n",
    "+----------+---------+\r\n",
    "|2025-05-28|Wednesday|\r\n",
    "+----------+---------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024a1845-4639-4583-a019-9591fa11b328",
   "metadata": {},
   "source": [
    "| Funkcja        | Typ  | Działa na       | Obcina do                                          | Przykład                                            |\r\n",
    "| -------------- | ---- | --------------- | -------------------------------------------------- | --------------------------------------------------- |\r\n",
    "| `trunc()`      | Data | `DateType`      | dzień / miesiąc / rok                              | `trunc(date_col, 'MM')` → pierwszy dzień miesiąca   |\r\n",
    "| `date_trunc()` | Czas | `TimestampType` | sekunda, minuta, godzina, dzień, miesiąc, rok itd. | `date_trunc('hour', timestamp_col)` → pełna godzina |\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1c4ee1-ef50-48fa-8491-277a6744d3a0",
   "metadata": {},
   "source": [
    "`trunc()` działa tylko na daty (DateType), nie timestampy!\n",
    "\n",
    "`date_trunc()` wymaga timestamp (albo to_timestamp() przed użyciem)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e507ca04-4efb-4bdf-9dee-e9242992e4e5",
   "metadata": {},
   "source": [
    "trunc(date, format)\n",
    "- \"MM\" – początek miesiąca\n",
    "\n",
    "- \"YYYY\" lub \"YY\" – początek roku\n",
    "\n",
    "date_trunc(unit, timestamp)\n",
    "- \"second\", \"minute\", \"hour\", \"day\", \"month\", \"year\", \"week\" itd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11dc507-8818-4a20-99c9-3c60421e6c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(\"2025-05-26\",)], [\"date_str\"])\n",
    "df = df.withColumn(\"date\", to_date(\"date_str\"))\n",
    "\n",
    "# Obetnij do pierwszego dnia miesiąca\n",
    "df.withColumn(\"month_start\", trunc(\"date\", \"MM\")).withColumn(\"year_start\", trunc(\"date\", \"YYYY\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c920a1de-cb06-4c09-8ccb-2a116b5069bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, date_trunc\n",
    "\n",
    "df = spark.range(1).select(current_timestamp().alias(\"now\"))\n",
    "\n",
    "# Obetnij do pełnej godziny\n",
    "df.withColumn(\"rounded_week\", date_trunc(\"week\", \"now\")).withColumn(\"rounded_hour\", date_trunc(\"hour\", \"now\")).withColumn(\"rounded_min\", date_trunc(\"minute\", \"now\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb76cea3-9d37-47f1-b756-352cd25623a7",
   "metadata": {},
   "source": [
    "### drop()  – Usuwanie kolumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b860c8-c415-4e9c-968b-6f1f50cd9400",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights_parquet.drop(\"DEST_COUNTRY_NAME\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d9bd5c-6eb5-4e83-8adb-9e7769e41a48",
   "metadata": {},
   "source": [
    "### dropna() - Usuwanie wierszy z null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13962e6e-a536-4502-8f76-901b5a891486",
   "metadata": {},
   "source": [
    "Jeśli chcesz usunąć wiersze, które zawierają null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368c0942-0b17-4ac5-85e4-16a5c8d56e16",
   "metadata": {},
   "source": [
    "- `df.dropna(how=\"any\")` - domyślnie: usuń wiersz, jeśli którakolwiek kolumna ma null\n",
    "- `df.dropna(how=\"all\")` - usuń wiersz, jeśli wszystkie kolumny mają null\n",
    "- `df.dropna(subset=[\"col1\"])` - tylko jeśli null w konkretnych kolumnach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e7ef80-53dc-4198-a94d-cbe9e43bca4d",
   "metadata": {},
   "source": [
    "### fillna() / fill() — Wypełnianie null\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a446fa-3d77-4383-9124-99d27ea989a7",
   "metadata": {},
   "source": [
    "```\n",
    "df.fillna(value)\r\n",
    "df.fillna(value, subset=[\"col1\", \"col2\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30273987-45d2-47c4-b4f3-2a40402176a3",
   "metadata": {},
   "source": [
    "Można też podać słownik z wartościami dla konkretnych kolumn:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39c7ff6-db9d-4aa7-a980-56daa499a2b2",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "df.fillna({\"col1\": \"brak danych\", \"col2\": 0})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3ea6a2-582e-409a-959b-c2161d350087",
   "metadata": {},
   "source": [
    "```\n",
    "df.fill(...)      # skrót\r\n",
    "df.fillna(...)    # pełna nazwa```\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa91a2f-befd-4a23-b668-e3359bf3b562",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"Marcin Prokop\", None), (None, 27),(\"Agnieszka Chylińska\",None)]\n",
    "\n",
    "columns = [\"name\", \"age\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b9a35d-5f98-4ef4-8e12-8d2bd60dc098",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=[\"age\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653e9d2d-5334-4ce7-b30a-2d241264f9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9618c2c0-02a9-4cc7-9abc-6f0e231b2073",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(6, subset = [\"age\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64addc79-2ab0-4433-8046-26f184e2f7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna({\"name\": \"brak danych\", \"age\": 0}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ff3bdc-63c4-4362-b3bc-0377254b02dd",
   "metadata": {},
   "source": [
    "### Złącza - jonis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346a1f2c-59f2-4d32-9c73-7852b68263ca",
   "metadata": {},
   "source": [
    "W PySpark funkcja join() służy do łączenia dwóch DataFrame'ów — dokładnie tak jak JOIN w SQL. Jest to jedna z najważniejszych operacji w przetwarzaniu danych."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b786d657-b129-43ac-88e6-f6de5a15445b",
   "metadata": {},
   "source": [
    "| Typ           | Opis                                  |\r\n",
    "| ------------- | ------------------------------------- |\r\n",
    "| `\"inner\"`     | tylko wspólne wiersze                 |\r\n",
    "| `\"left\"`      | wszystkie z `df1` + dopas. z `df2`    |\r\n",
    "| `\"right\"`     | wszystkie z `df2` + dopas. z `df1`    |\r\n",
    "| `\"outer\"`     | pełne złączenie (oba df-y)            |\r\n",
    "| `\"left_semi\"` | jak filtr – tylko `df1`, jeśli pasuje |\r\n",
    "| `\"left_anti\"` | tylko `df1`, jeśli **nie** pasuje     |\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5370d2-604e-406a-ace9-25275f78900d",
   "metadata": {},
   "source": [
    "**Składnia:**\n",
    "```\n",
    "df1.join(df2, on=warunek_lączenia, how=\"typ_join\")\r\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276ff8da-bcdb-4067-85f2-6160e7cdffdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "airportsna = (spark.read\n",
    "        .format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .option(\"sep\", \"\\t\")\n",
    "        .load(\"data/flights/airport-codes-na.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3878e74-bd3d-4cb4-a32f-9352e480b67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "departureDelays =(spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(\"data/flights/departuredelays.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53f6baf-a43a-4f98-b4ed-00cc4f68f23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "airportsna.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba37e7da-42e3-471e-b891-e74a2c1103b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "departureDelays.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a2ba10-93fe-4328-bcd9-5a531e085dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = airportsna.join(departureDelays,on=airportsna[\"IATA\"] == departureDelays[\"origin\"], how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd54cf94-75fb-4927-b839-7fd4e40508fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c0a79b-ca15-4ef6-a61d-a9ec5fec683b",
   "metadata": {},
   "source": [
    "`left_semi` \n",
    "- Zwraca tylko te wiersze z lewego DataFrame’a, dla których istnieje dopasowanie w prawym.\n",
    "\n",
    "- Nie zwraca kolumn z prawego DataFrame’a.\n",
    "\n",
    "- Działa jak filtr .filter() na podstawie istnienia wartości."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f055841f-d9ee-49d9-a22d-b2439fe42d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = spark.createDataFrame([\n",
    "    (\"Alice\", \"admin\"),\n",
    "    (\"Bob\", \"user\"),\n",
    "    (\"Charlie\", \"guest\"),\n",
    "    (\"Diana\", \"user\")\n",
    "], [\"name\", \"role\"])\n",
    "\n",
    "# Tabela uprawnień – dozwolone role\n",
    "# Pokaz przyklad  z duplikatami\n",
    "permissions = spark.createDataFrame([\n",
    "    (\"admin\",),\n",
    "    (\"user\",)\n",
    "], [\"role\"])\n",
    "\n",
    "result = users.join(permissions, on=\"role\", how=\"left_semi\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770c14a6-2be1-4e59-bcbd-32cc9a377caa",
   "metadata": {},
   "source": [
    "`left_semi` działa jak:\n",
    "```\n",
    "SELECT * FROM users WHERE role IN (SELECT role FROM permissions)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4282f763-40d1-4306-80f4-d943f9169e9b",
   "metadata": {},
   "source": [
    "| JOIN        | Zwraca kolumny z obu?   | Filtrowanie? | Szybkość                  |\r\n",
    "| ----------- | ----------------------- | ------------ | ------------------------- |\r\n",
    "| `inner`     | Tak                     | Nie          | wolniejszy                |\r\n",
    "| `left_semi` | Nie (tylko lewa tabela) | Tak          | szybszy (brak duplikacji) |\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3452d6-2953-4d52-b7c4-8989585f4990",
   "metadata": {},
   "source": [
    "Kiedy `left_semi` \n",
    "\n",
    "- Chcesz sprawdzić istnienie (czy coś pasuje)\n",
    "\n",
    "- Potrzebujesz tylko lewe dane\n",
    "\n",
    "- Robisz filtrowanie na podstawie innej tabeli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a509e2-39ad-4f01-bc72-a0b4c39de44a",
   "metadata": {},
   "source": [
    "`left_anti`\n",
    "\n",
    "Zwraca tylko te wiersze z lewej tabeli, które nie mają dopasowania w prawej tabeli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60abe309-580c-4abc-83b4-bfe0547fea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_permission = users.join(permissions, on=\"role\", how=\"left_anti\")\n",
    "no_permission.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0115b208-0bde-40f5-a0e5-b2fb0f08b4ab",
   "metadata": {},
   "source": [
    "`Charlie` to jedyny użytkownik, którego rola \"guest\" nie znajduje się w permissions, więc został zwrócony."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e8d766-cf79-482c-a1b2-98a3e0477ee6",
   "metadata": {},
   "source": [
    "| JOIN typ    | Zwraca jeśli dopasowanie? | Dane z prawej? | Przykład                   |\r\n",
    "| ----------- | ------------------------- | -------------- | -------------------------- |\r\n",
    "| `left_semi` | ✅ Tak                     | ❌ Nie          | „Kto ma uprawnienia”       |\r\n",
    "| `left_anti` | ❌ Nie                     | ❌ Nie          | „Kto **nie** ma uprawnień” |\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09656d5-2bdf-44fb-829f-1ca2fc507cd3",
   "metadata": {},
   "source": [
    "#### Zadanie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce3cd70-c7b3-443d-8e55-de7bc837454a",
   "metadata": {},
   "source": [
    "Wykonaj operację złączenia inner left right outer na poniższym dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5a4eb3-c98d-4598-a00d-655dbcc85fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame([\n",
    "    (1, ),\n",
    "    (1, ),\n",
    "    (1, ),\n",
    "    (2, ),\n",
    "    (3, ),\n",
    "    (3, ),\n",
    "    (3, ),\n",
    "], [\"nr\"])\n",
    "\n",
    "df2 = spark.createDataFrame([\n",
    "    (1, ),\n",
    "    (1, ),\n",
    "    (2, ),\n",
    "    (2, ),\n",
    "    (4, ),\n",
    "    (None, ),\n",
    "], [\"nr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6fc0b-bc7a-466e-b20e-8e4752bdf51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- inner; 3x2 -> 6 jedynek\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c1ccef-0080-4ee6-aac2-b5911859c679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- left\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae6b44d-f3ff-4d06-af9e-5ee86499c538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585e5fa6-0a77-4fa9-8453-eb534432cbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- left_semi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a4cfe7-cc9a-4b3c-a09e-faaa2e493dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- left_semi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948e41e8-1612-405a-8442-ad7be5014a90",
   "metadata": {},
   "source": [
    "### broadcast join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464f44bd-a74c-4989-89c8-0816cf360c09",
   "metadata": {},
   "source": [
    "- To technika, w której mała tabela (np. lookup, słownik, kody) jest rozsyłana (broadcastowana) do wszystkich węzłów klastra.\n",
    "\n",
    "- Spark może wtedy uniknąć shuffle (przemieszczania danych między węzłami) – działa szybciej.\n",
    "\n",
    "**Kiedy używać**:\n",
    "\n",
    "- Gdy jedna z tabel jest mała (np. < 10 MB).\n",
    "\n",
    "- Gdy masz join między:\n",
    "\n",
    "    - Dużym DataFrame (df_large)\n",
    "\n",
    "    - Małym DataFrame (df_small) z kluczami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee5b7a2-ae0e-4eef-b9af-e4e5f49a7e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duża tabela – np. dane transakcyjne\n",
    "df_large = spark.createDataFrame([\n",
    "    (1, \"A\"),\n",
    "    (2, \"B\"),\n",
    "    (3, \"C\"),\n",
    "    (4, \"D\"),\n",
    "], [\"id\", \"value\"])\n",
    "\n",
    "# Mała tabela – słownik opisów\n",
    "df_small = spark.createDataFrame([\n",
    "    (1, \"Jan\"),\n",
    "    (2, \"Anna\"),\n",
    "], [\"id\", \"name\"])\n",
    "\n",
    "# Broadcast join\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "joined = df_large.join(broadcast(df_small), on=\"id\", how=\"left\")\n",
    "joined.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262338af-dcf9-47a4-b4af-5e7154bb46b8",
   "metadata": {},
   "source": [
    "Zamiast przemieszczać dane obu stron **(shuffle join)**, Spark rozsyła małą tabelę do każdego węzła, więc duża tabela może być przetwarzana lokalnie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f56082-e992-47f0-9924-56c4313e73e8",
   "metadata": {},
   "source": [
    "- Spark sam może wybrać broadcast join, jeśli spełnione są warunki (mały rozmiar, **spark.sql.autoBroadcastJoinThreshold**).\n",
    "\n",
    "- Ale możesz wymusić **broadcast()** funkcją, jak powyżej.\n",
    "\n",
    "- Możesz zobaczyć plan zapytania **.explain()** – tam pojawi się **BroadcastHashJoin**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9035b295-b4ab-45b6-a635-1bd9acf71b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b86d5f1-af62-4288-8176-4a4b5166f758",
   "metadata": {},
   "source": [
    "### Union intersect subtract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb5ccec-f99e-417b-ab70-36c00df7ef3e",
   "metadata": {},
   "source": [
    "`union`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa5e6e8-ade5-4447-84ea-3a18147aa37d",
   "metadata": {},
   "source": [
    "- Łączy dwa DataFrame’y dokładając wiersze jeden pod drugi.\n",
    "\n",
    "- Zwraca wszystkie wiersze z obu DataFrame’ów.\n",
    "\n",
    "- Struktury obu DataFrame’ów muszą być zgodne (takie same kolumny i typy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1a9252-d918-44e9-8d27-ea929fed98b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame([\n",
    "    (1, \"A\"),\n",
    "    (2, \"B\")\n",
    "], [\"id\", \"value\"])\n",
    "\n",
    "df2 = spark.createDataFrame([\n",
    "    (2, \"B\"),\n",
    "    (3, \"C\")\n",
    "], [\"id\", \"value\"])\n",
    "\n",
    "union_df = df1.union(df2)\n",
    "union_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b356e52-942c-4097-a5c8-f951c502aaa7",
   "metadata": {},
   "source": [
    "**Uwaga**: union nie usuwa duplikatów. Jeśli chcesz je usunąć, użyj union_df.distinct()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca18c44-b2ee-44b9-9afd-d9cc2ab57fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "union_df.distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a0ba02-b5b7-4eea-a388-6e526cc1adec",
   "metadata": {},
   "source": [
    "`intersect`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014664a8-4008-4c82-9dc2-6b0db9b63f33",
   "metadata": {},
   "source": [
    "- Zwraca przecięcie dwóch DataFrame’ów — wiersze, które występują w obu.\n",
    "\n",
    "- Struktura też musi być zgodna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e8815f-f7c8-42e0-b99f-57a99ece3b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersect_df = df1.intersect(df2)\n",
    "intersect_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d53d7e-f84c-4a42-8459-6f53400c340c",
   "metadata": {},
   "source": [
    "`subtract`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a07cd2d-b875-4ede-ac54-17b47651a6f9",
   "metadata": {},
   "source": [
    "- Zwraca wiersze z pierwszego DataFrame, które nie występują w drugim.\n",
    "\n",
    "- Usuwa duplikaty (wynik unikalny)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2952633-ef35-483a-9e42-34b1a4802380",
   "metadata": {},
   "outputs": [],
   "source": [
    "subtract_df = df1.subtract(df2)\n",
    "subtract_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8a8ffa-a1bb-4d4b-815e-c227c5088d16",
   "metadata": {},
   "source": [
    "| Metoda      | Co zwraca                       | Duplikaty?      |\r\n",
    "| ----------- | ------------------------------- | --------------- |\r\n",
    "| `union`     | Wszystkie wiersze z obu tabel   | Nie (zachowuje) |\r\n",
    "| `intersect` | Wspólne wiersze (unikalne)      | Usuwa           |\r\n",
    "| `subtract`  | Wiersze z pierwszej bez drugiej | Usuwa           |\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbbcd2d-af5f-4479-8b3e-6c704b48145e",
   "metadata": {},
   "source": [
    "### distinct() vs dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a18d0de-a17a-43f9-a5b6-9c932432d2bc",
   "metadata": {},
   "source": [
    "`distinct()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4804a8be-f63c-4b93-9bbc-76e67cf82e2c",
   "metadata": {},
   "source": [
    "- Usuwa wszystkie duplikaty z DataFrame — zwraca unikalne wiersze.\n",
    "\n",
    "- Dotyczy wszystkich kolumn.\n",
    "\n",
    "- Nie pozwala na wskazanie konkretnych kolumn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c86308-f8d5-4f7b-8a92-ff9b7d01ee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (1, \"A\"),\n",
    "    (1, \"A\"),\n",
    "    (2, \"B\"),\n",
    "    (2, \"C\")\n",
    "], [\"id\", \"val\"])\n",
    "\n",
    "df.distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c6569f-4c7c-42d0-9f53-92477813179e",
   "metadata": {},
   "source": [
    "`dropDuplicates()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e22630-b204-4f97-8074-eec6a50674ab",
   "metadata": {},
   "source": [
    "- Usuwa duplikaty na podstawie wskazanych kolumn (jeśli podasz listę kolumn).\n",
    "\n",
    "- Jeśli nie podasz kolumn, działa jak distinct().\n",
    "\n",
    "- Pozwala zachować unikalność wg określonych kolumn, ignorując pozostałe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab1ac26-f499-461e-b277-c1702cf2fd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925bd220-fe8c-4d27-9a01-0a8de65e4df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropDuplicates([\"id\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c44e5ce-9c97-4223-a1d4-89dab67f5854",
   "metadata": {},
   "source": [
    "| Metoda             | Co robi?                             | Możliwość wskazania kolumn? |\r\n",
    "| ------------------ | ------------------------------------ | --------------------------- |\r\n",
    "| `distinct()`       | Usuwa duplikaty wg wszystkich kolumn | Nie                         |\r\n",
    "| `dropDuplicates()` | Usuwa duplikaty wg podanych kolumn   | Tak                         |\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e6ce04-936d-4242-88d5-ad4f12e83c75",
   "metadata": {},
   "source": [
    "### orderBy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68616131-6163-4571-a848-6454f82e03cb",
   "metadata": {},
   "source": [
    "Metoda orderBy() służy do sortowania wierszy w DataFrame według jednej lub wielu kolumn — rosnąco (domyślnie) lub malejąco.\n",
    "\n",
    "```\n",
    "df.orderBy(\"col1\")                          # Rosnąco (domyślnie)\r\n",
    "df.orderBy(df.col1.asc())                   # Jawnie rosnąco\r\n",
    "df.orderBy(df.col1.desc())                  # Malejąco\r\n",
    "df.orderBy(\"col1\", \"col2\")                  # Po kilku kolumnach\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7434bc39-a7c4-4bd9-8131-62d7538aac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights_parquet.orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d2a56b-7022-40ec-8985-e0e05797cde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights_parquet.orderBy(col(\"count\").asc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261f3a7b-dd73-470c-b68b-9168d540154c",
   "metadata": {},
   "source": [
    "### groupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b4b589-750e-45cf-8ca7-490e19b2a050",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_flights_parquet.groupBy(\"ORIGIN_COUNTRY_NAME\")\n",
    "  .sum(\"count\")\n",
    "  .withColumnRenamed(\"sum(count)\", \"total_flights\")\n",
    "  .orderBy(\"total_flights\", ascending=False)\n",
    "  .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683fec38-af48-4799-a6b3-98264c95a589",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_flights_parquet.groupBy(\"ORIGIN_COUNTRY_NAME\")\n",
    "  .agg({\"DEST_COUNTRY_NAME\": \"count\"})\n",
    "  .withColumnRenamed(\"count(DEST_COUNTRY_NAME)\", \"num_destinations\")\n",
    "  .orderBy(\"num_destinations\", ascending=False)\n",
    "  .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5672cf-0c3c-4cc3-bec8-e0b06cc95ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "(df_flights_parquet.groupBy(\"ORIGIN_COUNTRY_NAME\")\n",
    "  .agg(avg(\"count\").alias(\"avg_flights\"))\n",
    "  .orderBy(\"avg_flights\", ascending=False)\n",
    "  .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f25443-c0ba-4b2a-be8a-d12e2b494827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "(df_flights_parquet.groupBy(\"ORIGIN_COUNTRY_NAME\")\n",
    "  .agg(\n",
    "    f.sum(\"count\").alias(\"total_flights\"),\n",
    "    f.avg(\"count\").alias(\"avg_flights\"),\n",
    "    f.count(\"DEST_COUNTRY_NAME\").alias(\"num_destinations\")\n",
    "  )\n",
    "  .orderBy(f.desc(\"total_flights\"))\n",
    "  .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cb20d6-0f57-406c-b4cb-f1b660cf8cb4",
   "metadata": {},
   "source": [
    "#### Zadanie "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3142e383-18ab-4aed-ae3d-19e3da3d81f8",
   "metadata": {},
   "source": [
    "Z którego kraju jest najwięcej wylotów ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a7e978-ded2-4030-9d7f-481d42ea99fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ce59d5c-6fa8-4bef-ad24-7d576e8c59f7",
   "metadata": {},
   "source": [
    "**Przykładowy wynik:**\n",
    "\n",
    "```\n",
    "+--------------------+----------------+\r\n",
    "| ORIGIN_COUNTRY_NAME|num_destinations|\r\n",
    "+--------------------+----------------+\r\n",
    "|       United States|             125|\r\n",
    "|              Russia|               1|\r\n",
    "|            Anguilla|               1|\r\n",
    "|             Senegal|               1|\r\n",
    "|              Sweden|               1|\r\n",
    "|            Kiribati|               1|\r\n",
    "|              Guyana|               1|\r\n",
    "|         Philippines|               1|\r\n",
    "|           Singapore|               1|\r\n",
    "|            Malaysia|               1|\r\n",
    "|                Fiji|               1|\r\n",
    "|              Turkey|               1|\r\n",
    "|             Germany|               1|\r\n",
    "|         Afghanistan|               1|\r\n",
    "|              Jordan|               1|\r\n",
    "|               Palau|               1|\r\n",
    "|Turks and Caicos ...|               1|\r\n",
    "|              France|               1|\r\n",
    "|              Greece|               1|\r\n",
    "|British Virgin Is...|               1|\r\n",
    "+--------------------+----------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c7d92a-487b-4b6a-bf6a-7ef8b6a06faf",
   "metadata": {},
   "source": [
    "### describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3b1c2a-e709-421d-a3f2-3391254d4435",
   "metadata": {},
   "source": [
    "- Zwraca podstawowe statystyki opisowe dla wybranych kolumn liczbowych (i tekstowych) DataFrame.\n",
    "\n",
    "- Statystyki obejmują:\n",
    "\n",
    "    - count — liczba niepustych (non-null) wartości\n",
    "\n",
    "    - mean — średnia arytmetyczna\n",
    "    \n",
    "    - stddev — odchylenie standardowe\n",
    "    \n",
    "    - min — minimalna wartość (dla tekstu, alfabetycznie)\n",
    "    \n",
    "    - max — maksymalna wartość (dla tekstu, alfabetycznie)\n",
    " \n",
    "```\n",
    "df.describe()           # dla wszystkich kolumn\r\n",
    "df.describe(\"col1\", \"col2\")  # dla wybranych kolumn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1623e6-b4d5-489b-ac17-53ff1b16b5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flights_parquet.describe(\"count\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb95f314-cd91-4f9a-b6b0-874a2f409f3a",
   "metadata": {},
   "source": [
    "### CASE WHEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b83b87a-74a8-4840-aecb-2212f4069191",
   "metadata": {},
   "source": [
    "- Pozwala tworzyć nową kolumnę na podstawie warunków logicznych.\n",
    "\n",
    "- W Spark SQL używamy do tego funkcji when z pyspark.sql.functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869adec7-d942-4642-afdc-1618a0cb2121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "df_flights_parquet.withColumn(\n",
    "    \"traffic_category\",\n",
    "    when(col(\"count\") > 100, \"Wysokie obciążenie\")\n",
    "    .when(col(\"count\") > 50, \"Średnie obciążenie\")\n",
    "    .otherwise(\"Niskie obciążenie\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bed090f-3a70-4130-8cd8-4617eb1ffc07",
   "metadata": {},
   "source": [
    "### Funkcje oknowe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d17290-1e60-449d-9a82-73c026f3cd5b",
   "metadata": {},
   "source": [
    "Funkcje oknowe (ang. window functions) w PySpark pozwalają wykonywać operacje analityczne na grupach wierszy, z zachowaniem pełnego kontekstu danych (nie agregują danych do jednej wartości).\n",
    "\n",
    "\n",
    "- Działają w kontekście \"okna\" danych (czyli zdefiniowanego zbioru wierszy wokół aktualnego wiersza).\n",
    "\n",
    "- Nie redukują liczby wierszy (jak groupBy), tylko dodają kolumny z wynikami analizy.\n",
    "\n",
    "- Przykłady użycia:\n",
    "\n",
    "    - Rangi (row_number, rank)\n",
    "\n",
    "    - Agregaty (sum, avg, max, min)\n",
    "\n",
    "    - Operacje na sąsiednich wierszach (lead, lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0954ae-40e4-4781-ae65-7a37ecbd38bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "df = spark.createDataFrame([\n",
    "    (\"A\", 100, \"2025-01-01\"),\n",
    "    (\"A\", 100, \"2025-01-01\"),\n",
    "    (\"A\", 100, \"2025-01-01\"),\n",
    "    (\"A\", 200, \"2025-01-02\"),\n",
    "    (\"A\",  50, \"2025-01-03\"),\n",
    "    (\"B\",  30, \"2025-01-01\"),\n",
    "    (\"B\",  70, \"2025-01-02\"),\n",
    "    (\"C\",  10, \"2025-01-01\")\n",
    "], [\"group\", \"value\", \"date\"])\n",
    "\n",
    "# Konwersja na typ DateType\n",
    "df = df.withColumn(\"date\", f.to_date(\"date\", \"yyyy-MM-dd\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3011c1-5e63-4f0f-ab6d-4978ed46450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicja okna – partycjonowanie po kraju\n",
    "window_spec = Window.partitionBy(\"group\")\n",
    "\n",
    "df.withColumn(\"sum_in_group\", f.sum(\"value\").over(window_spec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c45b497-5f6d-4b87-8e10-b0757ee760fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy(\"group\").orderBy(\"date\")\n",
    "df_ranked = df \\\n",
    "    .withColumn(\"row_number\", f.row_number().over(window_spec)) \\\n",
    "    .withColumn(\"rank\", f.rank().over(window_spec)) \\\n",
    "    .withColumn(\"dense_rank\", f.dense_rank().over(window_spec))\n",
    "\n",
    "df_ranked.orderBy(\"group\", \"date\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f306a8d8-0729-4d42-b241-df467efd0e44",
   "metadata": {},
   "source": [
    "| Funkcja        | Opis                                                                            |\r\n",
    "| -------------- | ------------------------------------------------------------------------------- |\r\n",
    "| `row_number()` | Liczy unikalnie wiersze w kolejności. Bez powtórzeń.                            |\r\n",
    "| `rank()`       | Przy tych samych wartościach – przypisuje tę samą rangę, ale przeskakuje numer. |\r\n",
    "| `dense_rank()` | Jak `rank()`, ale bez przeskoków w numeracji.                                   |\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9906bfa-7679-48ed-81ce-137e84197056",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy(\"group\").orderBy(\"date\")\n",
    "df_lag_lead = df \\\n",
    "    .withColumn(\"prev_value\", f.lag(\"value\", 1).over(window_spec)) \\\n",
    "    .withColumn(\"next_value\", f.lead(\"value\", 1).over(window_spec))\n",
    "\n",
    "df_lag_lead.orderBy(\"group\", \"date\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc92e73-55ae-4d6b-ac85-a1626c7c5108",
   "metadata": {},
   "source": [
    "| Funkcja        | Co robi                                            |\r\n",
    "| -------------- | -------------------------------------------------- |\r\n",
    "| `lag(col, 1)`  | Zwraca wartość z **poprzedniego wiersza** w grupie |\r\n",
    "| `lead(col, 1)` | Zwraca wartość z **następnego wiersza** w grupie   |\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced0978a-d025-4a5a-bbbd-b4804aaa7603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# moving average (średnia krocząca) i moving sum (suma krocząca)\n",
    "\n",
    "window_spec = (\n",
    "    Window.partitionBy(\"group\").orderBy(\"date\")\n",
    "    .rowsBetween(Window.unboundedPreceding, 0) \n",
    ")\n",
    "df_moving = df \\\n",
    "    .withColumn(\"moving_sum\", f.sum(\"value\").over(window_spec)) \\\n",
    "    .withColumn(\"moving_avg\", f.avg(\"value\").over(window_spec))\n",
    "\n",
    "df_moving.orderBy(\"group\", \"date\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d026c1-eea5-4b02-b37b-83cbf051ff15",
   "metadata": {},
   "source": [
    "- Tylko poprzedni wiersz: .rowsBetween(-1, -1)\n",
    "\n",
    "- Poprzedni + bieżący + następny: .rowsBetween(-1, 1)\n",
    "\n",
    "- Od początku grupy do teraz: .rowsBetween(Window.unboundedPreceding, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbde852-7bc4-4ef4-863e-2e759a6c3aa5",
   "metadata": {},
   "source": [
    "#### Zadanie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef318348-2e72-4f5a-9f8b-41187e5c84be",
   "metadata": {},
   "source": [
    "**Analiza sprzedaży**\n",
    "Masz dane dziennej sprzedaży produktów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e827c3-30ab-4f3b-875a-efdb7827f1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (\"prodA\", \"2023-01-01\", 100),\n",
    "    (\"prodA\", \"2023-01-02\", 120),\n",
    "    (\"prodA\", \"2023-01-03\", 90),\n",
    "    (\"prodA\", \"2023-01-04\", 140),\n",
    "    (\"prodB\", \"2023-01-01\", 200),\n",
    "    (\"prodB\", \"2023-01-02\", 180),\n",
    "    (\"prodB\", \"2023-01-03\", 190),\n",
    "    (\"prodB\", \"2023-01-04\", 210),\n",
    "], [\"product\", \"date\", \"sales\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3267f70b-0c8c-4eef-b082-f4ec8b2c08b5",
   "metadata": {},
   "source": [
    "##### 1. Dodaj kolumnę row_number wg daty (dla każdego produktu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b0d3b4-cb9d-4ec6-bb85-fb62df79c69a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7985dfe2-aa59-4419-9707-39bdebc9bdaa",
   "metadata": {},
   "source": [
    "```\n",
    "+-------+----------+-----+----------+\r\n",
    "|product|      date|sales|row_number|\r\n",
    "+-------+----------+-----+----------+\r\n",
    "|  prodA|2023-01-01|  100|         1|\r\n",
    "|  prodA|2023-01-02|  120|         2|\r\n",
    "|  prodA|2023-01-03|   90|         3|\r\n",
    "|  prodA|2023-01-04|  140|         4|\r\n",
    "|  prodB|2023-01-01|  200|         1|\r\n",
    "|  prodB|2023-01-02|  180|         2|\r\n",
    "|  prodB|2023-01-03|  190|         3|\r\n",
    "|  prodB|2023-01-04|  210|         4|\r\n",
    "+-------+----------+-----+\n",
    "```----------+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e4721a-5e78-4719-8531-2dc4df903e87",
   "metadata": {},
   "source": [
    "##### 2. Dodaj kolumnę prev_sales z poprzednim dniem (funkcja lag)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f72b1f8-2609-4c1b-995b-c621c74ef795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d703e2e-2d3f-4098-8c5c-c374f0d04447",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "+-------+----------+-----+----------+\r\n",
    "|product|      date|sales|prev_sales|\r\n",
    "+-------+----------+-----+----------+\r\n",
    "|  prodA|2023-01-01|  100|      NULL|\r\n",
    "|  prodA|2023-01-02|  120|       100|\r\n",
    "|  prodA|2023-01-03|   90|       120|\r\n",
    "|  prodA|2023-01-04|  140|        90|\r\n",
    "|  prodB|2023-01-01|  200|      NULL|\r\n",
    "|  prodB|2023-01-02|  180|       200|\r\n",
    "|  prodB|2023-01-03|  190|       180|\r\n",
    "|  prodB|2023-01-04|  210|       190|\r\n",
    "+-------+----------+-----+----------+\r\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55e198d-1964-4399-91aa-aa5698ace22d",
   "metadata": {},
   "source": [
    "##### 3. Dodaj kolumnę next_sales z następnym dniem (lead)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51c5c6b-29c0-4e55-a791-2a9e48b44b21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ad09db6-6bd3-4b8b-a773-fabfe6db63c7",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "+-------+----------+-----+----------+\r\n",
    "|product|      date|sales|next_sales|\r\n",
    "+-------+----------+-----+----------+\r\n",
    "|  prodA|2023-01-01|  100|       120|\r\n",
    "|  prodA|2023-01-02|  120|        90|\r\n",
    "|  prodA|2023-01-03|   90|       140|\r\n",
    "|  prodA|2023-01-04|  140|      NULL|\r\n",
    "|  prodB|2023-01-01|  200|       180|\r\n",
    "|  prodB|2023-01-02|  180|       190|\r\n",
    "|  prodB|2023-01-03|  190|       210|\r\n",
    "|  prodB|2023-01-04|  210|      NULL|\r\n",
    "+-------+----------+-----+----------+\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97083e2-c3db-4d13-987c-3fb3c0d2cbe7",
   "metadata": {},
   "source": [
    "##### 4. Dodaj kolumnę moving_avg_3 – średnia z 3 dni: bieżący i 2 poprzednie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22ee2e5-6560-446a-8706-584de4e0d5e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87ecbf0e-c572-4f83-9645-fe34f26a8a6c",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "+-------+----------+-----+------------------+\r\n",
    "|product|      date|sales|      moving_avg_3|\r\n",
    "+-------+----------+-----+------------------+\r\n",
    "|  prodA|2023-01-01|  100|             100.0|\r\n",
    "|  prodA|2023-01-02|  120|             110.0|\r\n",
    "|  prodA|2023-01-03|   90|103.33333333333333|\r\n",
    "|  prodA|2023-01-04|  140|             112.5|\r\n",
    "|  prodB|2023-01-01|  200|             200.0|\r\n",
    "|  prodB|2023-01-02|  180|             190.0|\r\n",
    "|  prodB|2023-01-03|  190|             190.0|\r\n",
    "|  prodB|2023-01-04|  210|             195.0|\r\n",
    "+-------+----------+-----+------------------+\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca74fd5e-79cd-4ba2-8d7d-d3ac1ab87cd7",
   "metadata": {},
   "source": [
    "##### 5. Dodaj kolumnę diff_from_prev = różnica sales - prev_sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec17cab-b99c-4c35-a447-fe3c2e396be2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ba3f018-4cb0-46e5-9cd4-fc529cabad40",
   "metadata": {},
   "source": [
    "```\n",
    "+-------+----------+-----+--------------+\r\n",
    "|product|      date|sales|diff_from_prev|\r\n",
    "+-------+----------+-----+--------------+\r\n",
    "|  prodA|2023-01-01|  100|          NULL|\r\n",
    "|  prodA|2023-01-02|  120|            20|\r\n",
    "|  prodA|2023-01-03|   90|           -30|\r\n",
    "|  prodA|2023-01-04|  140|            50|\r\n",
    "|  prodB|2023-01-01|  200|          NULL|\r\n",
    "|  prodB|2023-01-02|  180|           -20|\r\n",
    "|  prodB|2023-01-03|  190|            10|\r\n",
    "|  prodB|2023-01-04|  210|            20|\r\n",
    "+-------+----------+-----+--------------+\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eabd34-4c91-4cbe-97ea-48c97a8cfb50",
   "metadata": {},
   "source": [
    "##### 6. Wyznacz datę z największą sprzedażą per produkt (użyj rank())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab1ad21-ff70-4816-88c0-c88c035a26bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2585e84-a26f-417f-97c6-05fd8cd5d772",
   "metadata": {},
   "source": [
    "```\n",
    "+-------+----------+-----+---------+\r\n",
    "|product|      date|sales|rank_desc|\r\n",
    "+-------+----------+-----+---------+\r\n",
    "|  prodA|2023-01-04|  140|        1|\r\n",
    "|  prodA|2023-01-02|  120|        2|\r\n",
    "|  prodA|2023-01-01|  100|        3|\r\n",
    "|  prodA|2023-01-03|   90|        4|\r\n",
    "|  prodB|2023-01-04|  210|        1|\r\n",
    "|  prodB|2023-01-01|  200|        2|\r\n",
    "|  prodB|2023-01-03|  190|        3|\r\n",
    "|  prodB|2023-01-02|  180|        4|\r\n",
    "+-------+----------+-----+---------+\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea59b544-e3d6-4042-901c-e1ce3b2b9dcc",
   "metadata": {},
   "source": [
    "### Zapisywanie DataFrame w PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890e7ec8-8957-4cc4-a027-a40fe23f0482",
   "metadata": {},
   "source": [
    "`csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c179424-fc3a-42b2-91c2-933e427fac9d",
   "metadata": {},
   "source": [
    "Przydatne opcje:\n",
    "- `header=True` — dodaje nagłówki kolumn do pliku CSV\n",
    "\n",
    "- `mode` — tryb zapisu:\n",
    "\n",
    "    - `\"overwrite\"` — nadpisuje pliki w katalogu\n",
    "\n",
    "    - `\"append\"` — dopisuje do istniejących plików\n",
    "\n",
    "    - `\"ignore\"` — ignoruje zapis, jeśli katalog istnieje\n",
    "\n",
    "    - `\"error\"` lub \"errorifexists\" — domyślny, wyrzuca błąd jeśli katalog istnieje\n",
    "\n",
    "- `sep=\",\"` — separator kolumn (domyślnie przecinek, można zmienić np. na tabulator \\t)\n",
    "\n",
    "- `quote` — znak cudzysłowu (np. \")\n",
    "\n",
    "- `escape` — znak ucieczki (np. \\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469fbf36-881a-4a96-8646-fed3fa9577b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c8ce15-597d-4d02-b41d-c747d0bb0ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.write\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"sep\", \";\")\n",
    "  .mode(\"overwrite\")\n",
    "  .csv(\"output/csv_folder/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c07e36d-4397-4fbb-ba88-bc690b3d52ef",
   "metadata": {},
   "source": [
    "`parquet`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a621ca19-8fa3-4dc4-913f-dacf52ff0850",
   "metadata": {},
   "source": [
    "Parquet to kolumnowy, skompresowany format, wydajny i popularny w big data.\n",
    "\n",
    "\n",
    "- `mode` — analogicznie jak w CSV (`overwrite`, `append` itd.)\n",
    "\n",
    "- `compression` — typ kompresji (`snappy` — domyślna, `gzip`, `none`, `brotli`, `lz4` itd.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07c0327-ca6d-4354-b31f-1c65f67ef863",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .option(\"compression\", \"gzip\") \\\n",
    "  .parquet(\"output/parquet_folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f666c7-af04-4831-b371-a972ce20ad0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2191202f-c91e-48d8-8db8-861ed479bfe0",
   "metadata": {},
   "source": [
    "#### Zapis do pojedynczego pliku:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076b74b0-d23e-4106-9612-1b3b97f39fb5",
   "metadata": {},
   "source": [
    "Domyślnie Spark zapisuje wiele plików (po jednym na partycję). Aby wymusić pojedynczy plik:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb99ea1-e094-44b8-9032-349af3074f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.coalesce(1)\n",
    "     .write\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"sep\", \";\")\n",
    "    .mode(\"overwrite\")\n",
    "    .csv(\"output/csv_folder/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5b6377-8c5d-425f-abbf-d96c7e9b1b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.coalesce(1)\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"compression\", \"gzip\")\n",
    "    .parquet(\"output/parquet_folder\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1775c03a-e8ec-4af6-abb1-ac441714a952",
   "metadata": {},
   "source": [
    "### UDF vs Pandas UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870b1ebd-b9ec-4211-907d-a45f87e6fee4",
   "metadata": {},
   "source": [
    "W PySpark istnieją dwa główne podejścia do definiowania funkcji użytkownika: tradycyjne funkcje użytkownika (UDF) oraz funkcje użytkownika oparte na Pandas (pandas UDF). Poniżej przedstawiam porównanie obu metod:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e553b72e-ce71-435e-8160-a636a7e37701",
   "metadata": {},
   "source": [
    "- **Definicja**: Tworzone za pomocą dekoratora @pandas_udf, operują na kolumnach jako obiektach pandas.Series.\n",
    "\n",
    "- **Wydajność**: Bardziej wydajne dzięki wykorzystaniu wektorowych operacji Pandas i Apache Arrow do szybkiego transferu danych między JVM a Pythonem.\n",
    "\n",
    "- **Zastosowanie**: Idealne do operacji, które można wyrazić jako operacje wektorowe na danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469767f0-8162-487d-a635-f5cce5ec95c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcaab8f-600f-45ea-aee8-d518ba49c0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1,), (2,), (3,)]\n",
    "df = spark.createDataFrame(data, [\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1253f4f7-1d27-4240-a0c9-cb792a2d17ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zdefiniowanie funkcji cubed()\n",
    "@udf(IntegerType())\n",
    "def cubed(s):\n",
    "    return s*s*s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b41239-78b2-4956-9a10-c31a1f0f70b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_udf = df.withColumn(\"cubed_value\", cubed(df[\"value\"]))\n",
    "df_udf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7902e5-9dbd-406f-ab30-0745e6bfc53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "@pandas_udf(\"int\")\n",
    "def cubed_pandas(a: pd.Series) -> pd.Series:\n",
    "    return a**3\n",
    "\n",
    "\n",
    "df_pandas_udf = df.withColumn(\"cubed_value\", cubed_pandas(df[\"value\"]))\n",
    "df_pandas_udf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce14c88-4c1e-4e7e-87fa-cfe05da761d6",
   "metadata": {},
   "source": [
    "| Cecha                       | Tradycyjne UDF        | pandas UDF                 |                                                                     |\r\n",
    "| --------------------------- | --------------------- | -------------------------- | ------------------------------------------------------------------- |\r\n",
    "| **Typ operacji**            | Wiersz po wierszu     | Wektorowe (kolumnowe)      |                                                                     |\r\n",
    "| **Wydajność**               | Niższa                | Wyższa dzięki wektoryzacji |                                                                     |\r\n",
    "| **Wsparcie dla Arrow**      | Nie                   | Tak                        |                                                                     |\r\n",
    "| **Złożoność implementacji** | Prosta                | Wymaga znajomości Pandas   |                                                                     |\r\n",
    "| **Zastosowanie**            | Niestandardowa logika | Operacje wektorowe         | ([Dokumentacja Databricks][1], [en.wikipedia.org][2], [Brainly][3]) |\r\n",
    "\r\n",
    "[1]: https://docs.databricks.com/aws/en/udf/pandas?utm_source=chatgpt.com \"A pandas user-defined function (UDF) - Databricks Documentation\"\r\n",
    "[2]: https://en.wikipedia.org/wiki/User-defined_function?utm_source=chatgpt.com \"User-defined function\"\r\n",
    "[3]: https://brainly.com/question/47363257?utm_source=chatgpt.com \"What is the difference between a built-in function and a user-defined ...\"\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
